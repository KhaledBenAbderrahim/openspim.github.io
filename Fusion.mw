= Fusion = 

[[Registration|'''Registered''']] multi-view OpenSPIM data need to be fused into a single output image in order to achieve complete coverage of a large specimen. Fusion means here combining information from different views in areas where the views overlap. Several strategies to do so exist, many are published (ADD REFS LATER). We will focus here on the two fusion methods implemented in Fiji - the content based multiview fusion and multiview deconvolution.


''Note that fused data are different, not necessarily better compared to raw SPIM data. Both fusion algorithms described here potentially deteriorate the quality of the data in some respects while improving other aspects. We will discuss the fusion artefacts in the respective sections. However it should be said that sometimes it is beneficial to NOT fuse the data at all and perform analysis on the raw registered image stacks (for example segmentation of cells in the individual views and reconciliation of the results in the segmentation domain). In the section on of this tutorial on [[Browsing|'''browsing''']] we will describe how to view raw registered multi-view OpenSPIM views.

= Content Based Fusion =

The content based multi-view fusion evaluates local information entropy in the areas where several views overlap and combines the views by enhancing  the low entropy information from the view containing useful data while suppressing the high entropy noise from the blurred data in other views. The principles of the method are discussed in depth [http://fiji.sc/SPIM_Registration_Method#Image_Fusion_and_blending '''here'''] and the parameters of the Fiji plugin implementing the method are described [http://fiji.sc/Multi-View_Fusion '''here''']. As before we enhance these technical description with tutorial style walk through using the [[Raw_data|sample OpenSPIM data]]. 

Content based multi-view fusion requires significant [[Pre-requisites|computational resources]]. The input raw data stacks are large and when they are transformed to the position where they overlap, the bounding box of the output volume can become several fold larger. To process on such large volumes can take significant amount of time and it may fail due to insufficient memory even on the largest computer systems (we did experience out of memory exceptions on a system with 128GB of RAM!). Therefore we will proceed sequentially, minimising the memory footprint.

* first we will fuse '''four times down-sampled''' data with all computationally demanding options turned off - see [[Fusion#First_approximate_run|'''First approximate run''']].
* next we will '''crop''' the output volume to include only the specimen - see [[Fusion#Cropping|'''Cropping''']].
* finally we will fuse the '''cropped''' volume with all options turned on - see [[Fusion#Final_run|'''Final run''']].

== First approximate run ==

=== Input ===

===  Run ===

=== Output ===

== Cropping ==

== Final run ==

= Deconvolution =

== Scaling of input data ==

== Debug run ==

== Final run ==

== Using GPUs ==
